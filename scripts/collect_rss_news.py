#!/usr/bin/env python3
"""
RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å­ä¾›é–¢é€£äº‹ä»¶ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å®Ÿéš›ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹URLã‚’å–å¾—ã—ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
"""

import os
import json
import feedparser
from datetime import datetime
from urllib.parse import urlparse

# ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
DATA_DIR = 'data'
CHILD_CASES_FILE = os.path.join(DATA_DIR, 'child-cases.json')

# RSSãƒ•ã‚£ãƒ¼ãƒ‰ä¸€è¦§ï¼ˆæ—¥æœ¬ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ï¼‰
RSS_FEEDS = [
    {"url": "https://news.google.com/rss/search?q=å…ç«¥è™å¾…&hl=ja&gl=JP&ceid=JP:ja", "source": "Google News"},
    {"url": "https://news.google.com/rss/search?q=å­ã©ã‚‚+äº‹ä»¶&hl=ja&gl=JP&ceid=JP:ja", "source": "Google News"},
    {"url": "http://www3.nhk.or.jp/rss/news/cat0.xml", "source": "NHK"},
    {"url": "https://mainichi.jp/rss/etc/mainichi-flash.rss", "source": "æ¯æ—¥æ–°è"},
    {"url": "https://www.asahi.com/rss/asahi/newsheadlines.rdf", "source": "æœæ—¥æ–°è"},
]

# ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå°‘ãªãã¨ã‚‚1ã¤å¿…é ˆï¼‰- ç·©å’Œ
MAIN_KEYWORDS = ["è™å¾…", "ãƒã‚°ãƒ¬ã‚¯ãƒˆ", "å…ç«¥", "å­ã©ã‚‚", "å­ä¾›", "å…ç«¥ç›¸è«‡æ‰€", "ä¿è­·", "å¹¼å…", "ä¹³å…", "å°å­¦ç”Ÿ", "åœ’å…"]

# ã‚µãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆäº‹ä»¶æ€§ã‚’ç¤ºã™ï¼‰
SUB_KEYWORDS = ["å‚·å®³", "é€®æ•", "æ­»äº¡", "æš´è¡Œ", "äº‹ä»¶", "å®¹ç–‘", "é€æ¤œ", "èµ·è¨´", "æ®ºå®³", "éºä½“"]

def load_existing_data():
    """æ—¢å­˜ã®JSONãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    if os.path.exists(CHILD_CASES_FILE):
        with open(CHILD_CASES_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []

def save_data(data):
    """JSONãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    with open(CHILD_CASES_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"âœ… Saved {len(data)} items to {CHILD_CASES_FILE}")

def is_relevant_article(title, summary=""):
    """è¨˜äº‹ãŒå­ä¾›é–¢é€£äº‹ä»¶ã«é–¢ä¿‚ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    text = (title + " " + summary).lower()
    
    # ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒ1ã¤ä»¥ä¸Šå«ã¾ã‚Œã¦ã„ã‚‹ã‹
    has_main = any(kw in text for kw in MAIN_KEYWORDS)
    
    if not has_main:
        return False
    
    # ã‚µãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚å«ã¾ã‚Œã¦ã„ã‚Œã°ã‚ˆã‚Šç¢ºå®Ÿ
    has_sub = any(kw in text for kw in SUB_KEYWORDS)
    
    # ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã ã‘ã§ã‚‚OKã€ã‚µãƒ–ãŒã‚ã‚Œã°ãƒœãƒ¼ãƒŠã‚¹
    return True

def extract_tags(title, summary=""):
    """ã‚¿ã‚¤ãƒˆãƒ«ã¨æ¦‚è¦ã‹ã‚‰ã‚¿ã‚°ã‚’æŠ½å‡º"""
    text = title + " " + summary
    tags = []
    
    # ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ã‚¿ã‚°æŠ½å‡º
    for kw in MAIN_KEYWORDS:
        if kw in text:
            tags.append(kw)
    
    # ã‚µãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ã‚¿ã‚°æŠ½å‡º
    for kw in SUB_KEYWORDS:
        if kw in text:
            tags.append(kw)
    
    # åŠ å®³è€…ã‚¿ã‚°
    if "çˆ¶" in text or "çˆ¶è¦ª" in text:
        tags.append("å®Ÿçˆ¶")
    if "æ¯" in text or "æ¯è¦ª" in text:
        tags.append("å®Ÿæ¯")
    if "ç¶™çˆ¶" in text:
        tags.append("ç¶™çˆ¶")
    if "ç¶™æ¯" in text:
        tags.append("ç¶™æ¯")
    if "äº¤éš›ç›¸æ‰‹" in text:
        tags.append("äº¤éš›ç›¸æ‰‹")
    
    return list(set(tags))  # é‡è¤‡é™¤å»

def parse_date(entry):
    """RSSã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º"""
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        return datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d')
    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
        return datetime(*entry.updated_parsed[:6]).strftime('%Y-%m-%d')
    return datetime.now().strftime('%Y-%m-%d')

def collect_from_rss():
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’åé›†"""
    print("ğŸ“¡ RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å­ä¾›é–¢é€£äº‹ä»¶ã‚’åé›†ä¸­...")
    
    existing = load_existing_data()
    existing_urls = {item.get('url', '') for item in existing}
    
    new_items = []
    max_id = max([item.get('id', 0) for item in existing], default=0)
    
    for feed_info in RSS_FEEDS:
        try:
            print(f"  ğŸ” {feed_info['source']}...")
            feed = feedparser.parse(feed_info['url'])
            
            for entry in feed.entries:
                title = entry.get('title', '')
                summary = entry.get('summary', entry.get('description', ''))
                url = entry.get('link', '')
                
                # æ—¢å­˜URLã¯ã‚¹ã‚­ãƒƒãƒ—
                if url in existing_urls:
                    continue
                
                # é–¢é€£è¨˜äº‹ã‹ãƒã‚§ãƒƒã‚¯
                if not is_relevant_article(title, summary):
                    continue
                
                max_id += 1
                new_item = {
                    "id": max_id,
                    "date": parse_date(entry),
                    "title": title[:50],  # 50æ–‡å­—ã«åˆ¶é™
                    "summary": summary[:150] if summary else title,  # 150æ–‡å­—ã«åˆ¶é™
                    "url": url,
                    "source": feed_info['source'],
                    "tags": extract_tags(title, summary),
                    "collected_at": datetime.now().isoformat()
                }
                new_items.append(new_item)
                existing_urls.add(url)
                
        except Exception as e:
            print(f"  âŒ {feed_info['source']}ã‚¨ãƒ©ãƒ¼: {e}")
    
    if new_items:
        existing.extend(new_items)
        save_data(existing)
        print(f"âœ… {len(new_items)}ä»¶ã®æ–°ã—ã„è¨˜äº‹ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    else:
        print("â„¹ï¸ æ–°ã—ã„è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    return len(new_items)

def main():
    print("=" * 50)
    print(f"ğŸ¤– RSSåé›†é–‹å§‹: {datetime.now().isoformat()}")
    print("=" * 50)
    
    os.makedirs(DATA_DIR, exist_ok=True)
    collect_from_rss()
    
    print("=" * 50)
    print("âœ… RSSåé›†å®Œäº†!")
    print("=" * 50)

if __name__ == "__main__":
    main()
